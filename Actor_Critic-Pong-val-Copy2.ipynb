{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4)\n",
      "6\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gym, os\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torchvision.transforms as T\n",
    "from wrappers import *\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = make_env(env)\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "print(env.observation_space.shape)\n",
    "action_space = env.action_space.n\n",
    "print(action_space)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 3000\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def get_screen(x):\n",
    "    state = np.array(x)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    state = torch.from_numpy(state)\n",
    "    return state.unsqueeze(0)\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two seperate networks the actor and the critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_out = 64\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_frames, h, w, outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames, 32, kernel_size=5, stride=2)\n",
    "        # self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=2)\n",
    "        # self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, end_out, kernel_size=5, stride=2)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * end_out\n",
    "        self.down = nn.Linear(linear_input_size, 512)\n",
    "        self.a = nn.Linear(512, outputs)\n",
    "        self.c = nn.Linear(512, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.down(x.view(x.size(0), -1))\n",
    "        a = self.a(x)\n",
    "        c = x = self.c(x)\n",
    "        \n",
    "        a = Categorical(F.softmax(a, dim=-1))\n",
    "        return a, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_frames, h, w, outputs):\n",
    "        super(Critic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames, 8, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, end_out, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(end_out)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * end_out\n",
    "        self.out = nn.Linear(linear_input_size, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.out(x.view(x.size(0), -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Score avg: -21.0, Loss: 12.059379577636719\n",
      "Iteration: 1, Score avg: -20.0, Loss: 14.80235481262207\n",
      "Iteration: 2, Score avg: -21.0, Loss: 13.2349214553833\n",
      "Iteration: 3, Score avg: -21.0, Loss: 13.640836715698242\n",
      "Iteration: 4, Score avg: -21.0, Loss: 14.33635425567627\n",
      "Iteration: 5, Score avg: -20.0, Loss: 14.084345817565918\n",
      "Iteration: 6, Score avg: -20.0, Loss: 14.342296600341797\n",
      "Iteration: 7, Score avg: -19.0, Loss: 19.17304039001465\n",
      "Iteration: 8, Score avg: -20.0, Loss: 19.73275375366211\n",
      "Iteration: 9, Score avg: -21.0, Loss: 14.925650596618652\n",
      "Iteration: 10, Score avg: -21.0, Loss: 15.513360977172852\n",
      "Iteration: 11, Score avg: -21.0, Loss: 15.023296356201172\n",
      "Iteration: 12, Score avg: -20.0, Loss: 15.294929504394531\n",
      "Iteration: 13, Score avg: -20.0, Loss: 14.152772903442383\n",
      "Iteration: 14, Score avg: -20.0, Loss: 12.161042213439941\n",
      "Iteration: 15, Score avg: -20.0, Loss: 9.801424980163574\n",
      "Iteration: 16, Score avg: -20.0, Loss: 7.49735689163208\n",
      "Iteration: 17, Score avg: -21.0, Loss: 4.731375694274902\n",
      "Iteration: 18, Score avg: -21.0, Loss: 3.2494471073150635\n",
      "Iteration: 19, Score avg: -21.0, Loss: 2.2732677459716797\n",
      "Iteration: 20, Score avg: -21.0, Loss: 1.5100674629211426\n",
      "Iteration: 21, Score avg: -21.0, Loss: 1.0272274017333984\n",
      "Iteration: 22, Score avg: -21.0, Loss: 0.7536191940307617\n",
      "Iteration: 23, Score avg: -21.0, Loss: 0.5938447117805481\n",
      "Iteration: 24, Score avg: -21.0, Loss: 0.45921674370765686\n",
      "Iteration: 25, Score avg: -21.0, Loss: 0.38487088680267334\n",
      "Iteration: 26, Score avg: -21.0, Loss: 0.33914923667907715\n",
      "Iteration: 27, Score avg: -21.0, Loss: 0.30468520522117615\n",
      "Iteration: 28, Score avg: -21.0, Loss: 0.2814263701438904\n",
      "Iteration: 29, Score avg: -21.0, Loss: 0.24095551669597626\n",
      "Iteration: 30, Score avg: -21.0, Loss: 0.20270347595214844\n",
      "Iteration: 31, Score avg: -21.0, Loss: 0.17638957500457764\n",
      "Iteration: 32, Score avg: -21.0, Loss: 0.17255015671253204\n",
      "Iteration: 33, Score avg: -21.0, Loss: 0.18738603591918945\n",
      "Iteration: 34, Score avg: -21.0, Loss: 0.208395853638649\n",
      "Iteration: 35, Score avg: -21.0, Loss: 0.21661315858364105\n",
      "Iteration: 36, Score avg: -21.0, Loss: 0.20584452152252197\n",
      "Iteration: 37, Score avg: -21.0, Loss: 0.1855221539735794\n",
      "Iteration: 38, Score avg: -21.0, Loss: 0.17187567055225372\n",
      "Iteration: 39, Score avg: -21.0, Loss: 0.1736922711133957\n",
      "Iteration: 40, Score avg: -21.0, Loss: 0.18512605130672455\n",
      "Iteration: 41, Score avg: -21.0, Loss: 0.19642840325832367\n",
      "Iteration: 42, Score avg: -21.0, Loss: 0.19612246751785278\n",
      "Iteration: 43, Score avg: -21.0, Loss: 0.18489541113376617\n",
      "Iteration: 44, Score avg: -21.0, Loss: 0.1742488443851471\n",
      "Iteration: 45, Score avg: -21.0, Loss: 0.17098045349121094\n",
      "Iteration: 46, Score avg: -21.0, Loss: 0.17725367844104767\n",
      "Iteration: 47, Score avg: -21.0, Loss: 0.1848798543214798\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ba1788695d11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#env.render()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#state = torch.FloatTensor(state).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-08da6a6c650f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_rate = 1e-4\n",
    "n_iters = 5000\n",
    "avg_over = 1\n",
    "val_loss_coeff = 1\n",
    "ent_loss_coeff = 0.01\n",
    "\n",
    "init_screen = get_screen(env.reset())\n",
    "_, num_frames, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "actor_critic = ActorCritic(num_frames, screen_height, screen_width, n_actions).to(device)\n",
    "critic = Critic(num_frames, screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "optimizer = optim.Adam(actor_critic.parameters(), lr=learn_rate)\n",
    "optimizerC = optim.Adam(critic.parameters(), lr=learn_rate)\n",
    "\n",
    "score_avg = []\n",
    "ac_loss_list = []\n",
    "\n",
    "\n",
    "for iter in range(n_iters):\n",
    "    state = get_screen(env.reset())\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        #env.render()\n",
    "        #state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = actor_critic(state)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        next_state = get_screen(next_state)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            #print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "            score_avg.append(reward_sum)\n",
    "            break\n",
    "    \n",
    "        \n",
    "    \n",
    "    #next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = actor_critic(next_state)\n",
    "    returns = compute_returns(next_value, rewards, masks)\n",
    "\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    returns = torch.cat(returns).detach()\n",
    "    values = torch.cat(values)\n",
    "\n",
    "    advantage = returns - values\n",
    "    \n",
    "    actor_loss = (-log_probs * advantage.detach()).mean()\n",
    "    critic_loss = (advantage.pow(2)).mean()\n",
    "    ent_loss = entropy\n",
    "\n",
    "    ac_loss = actor_loss+val_loss_coeff*critic_loss+ent_loss_coeff*ent_loss\n",
    "    print('Iteration: {}, Score avg: {}, Loss: {}'.format(iter, reward_sum, ac_loss))\n",
    "    \n",
    "    \n",
    "    ac_loss_list.append(ac_loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ac_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5\n",
    "for iter in range(n_iter):\n",
    "    state = get_screen(env.reset())\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        env.render()\n",
    "        #state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = actor_critic(state)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        next_state = get_screen(next_state)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor, 'results/actor.pkl')\n",
    "torch.save(critic, 'results/critic.pkl')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
