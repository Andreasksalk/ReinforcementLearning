{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4)\n",
      "6\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gym, os\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Multinomial\n",
    "import torchvision.transforms as T\n",
    "from wrappers import *\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "import statistics\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"Pong-v0\")\n",
    "env = make_env(env)\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "print(env.observation_space.shape)\n",
    "action_space = env.action_space.n\n",
    "print(action_space)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 3000\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def process_rollout(gamma = 0.99, lambd = 1.0, num_workers = 1):\n",
    "    _, _, _, _, last_values = steps[-1]\n",
    "    returns = last_values.data\n",
    "\n",
    "    advantages = torch.zeros(num_workers, 1)\n",
    "    #if cuda: advantages = advantages.cuda()\n",
    "\n",
    "    out = [None] * (len(steps) - 1)\n",
    "\n",
    "    # run Generalized Advantage Estimation, calculate returns, advantages\n",
    "    for t in reversed(range(len(steps) - 1)):\n",
    "        rewards, masks, actions, policies, values = steps[t]\n",
    "        _, _, _, _, next_values = steps[t + 1]\n",
    "\n",
    "        returns = rewards + returns * gamma * masks\n",
    "\n",
    "        deltas = rewards + next_values.data * gamma * masks - values.data\n",
    "        advantages = advantages * gamma * lambd * masks + deltas\n",
    "\n",
    "        out[t] = actions, policies, values, returns, advantages\n",
    "\n",
    "    # return data as batched Tensors, Variables\n",
    "    return map(lambda x: torch.cat(x, 0), zip(*out))\n",
    "\n",
    "def get_screen(x):\n",
    "    state = np.array(x)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    state = torch.from_numpy(state)\n",
    "    state = state.float() / 255\n",
    "    return state.unsqueeze(0)\n",
    "\n",
    "def ortho_weights(shape, scale=1.):\n",
    "    \"\"\" PyTorch port of ortho_init from baselines.a2c.utils \"\"\"\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        flat_shape = shape[1], shape[0]\n",
    "    elif len(shape) == 4:\n",
    "        flat_shape = (np.prod(shape[1:]), shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    a = np.random.normal(0., 1., flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.transpose().copy().reshape(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        return torch.from_numpy((scale * q).astype(np.float32))\n",
    "    if len(shape) == 4:\n",
    "        return torch.from_numpy((scale * q[:, :shape[1], :shape[2]]).astype(np.float32))\n",
    "\n",
    "def game_initializer(module):\n",
    "    \"\"\" Parameter initializer for Atari models\n",
    "    Initializes Linear, Conv2d, and LSTM weights.\n",
    "    \"\"\"\n",
    "    classname = module.__class__.__name__\n",
    "\n",
    "    if classname == 'Linear':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'Conv2d':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'LSTM':\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'weight_hh' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'bias' in name:\n",
    "                param.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two seperate networks the actor and the critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_out = 64\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_frames, outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames, 32, kernel_size=8, stride=4)\n",
    "        # self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, end_out, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.down = nn.Linear(7*7*64, 512)\n",
    "        self.a = nn.Linear(512, outputs)\n",
    "        self.c = nn.Linear(512, 1)\n",
    "        \n",
    "        self.apply(game_initializer)\n",
    "        self.a.weight.data = ortho_weights(self.a.weight.size(), scale=.01)\n",
    "        self.c.weight.data = ortho_weights(self.c.weight.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.down(x.view(x.size(0), -1))\n",
    "        a = self.a(x)\n",
    "        c = self.c(x)\n",
    "        return a, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:72: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:85: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8800, Score avg: -33.24, Loss: 145.86846923828125\n",
      "Iteration: 8850, Score avg: -20.44, Loss: 162.73435974121094\n",
      "Iteration: 8900, Score avg: -20.6, Loss: 113.18492126464844\n",
      "Iteration: 8950, Score avg: -20.5, Loss: 108.09701538085938\n",
      "Iteration: 9000, Score avg: -20.46, Loss: 101.59829711914062\n",
      "Iteration: 9050, Score avg: -20.64, Loss: 84.47756958007812\n",
      "Iteration: 9100, Score avg: -20.78, Loss: 104.55838775634766\n",
      "Iteration: 9150, Score avg: -20.5, Loss: 155.05332946777344\n",
      "Iteration: 9200, Score avg: -20.64, Loss: 129.76495361328125\n",
      "Iteration: 9250, Score avg: -20.62, Loss: 58.61138153076172\n",
      "Iteration: 9300, Score avg: -20.5, Loss: 98.97052001953125\n",
      "Iteration: 9350, Score avg: -20.46, Loss: 174.5591278076172\n",
      "Iteration: 9400, Score avg: -20.32, Loss: 121.80577850341797\n",
      "Iteration: 9450, Score avg: -20.54, Loss: 82.45608520507812\n",
      "Iteration: 9500, Score avg: -20.42, Loss: 82.1850357055664\n",
      "Iteration: 9550, Score avg: -20.54, Loss: 70.83219909667969\n",
      "Iteration: 9600, Score avg: -20.44, Loss: 96.6781234741211\n",
      "Iteration: 9650, Score avg: -20.38, Loss: 194.4857635498047\n",
      "Iteration: 9700, Score avg: -20.42, Loss: 154.34654235839844\n",
      "Iteration: 9750, Score avg: -20.5, Loss: 156.71360778808594\n",
      "Iteration: 9800, Score avg: -20.46, Loss: 26.867624282836914\n",
      "Iteration: 9850, Score avg: -20.68, Loss: 36.2335319519043\n",
      "Iteration: 9900, Score avg: -20.48, Loss: 58.58790588378906\n",
      "Iteration: 9950, Score avg: -20.36, Loss: 123.65978240966797\n",
      "Iteration: 10000, Score avg: -20.5, Loss: 108.15918731689453\n",
      "Iteration: 10050, Score avg: -20.5, Loss: 102.86430358886719\n",
      "Iteration: 10100, Score avg: -20.58, Loss: 112.8406753540039\n",
      "Iteration: 10150, Score avg: -20.52, Loss: 151.55331420898438\n",
      "Iteration: 10200, Score avg: -20.54, Loss: 157.30337524414062\n",
      "Iteration: 10250, Score avg: -20.4, Loss: 191.95094299316406\n",
      "Iteration: 10300, Score avg: -20.52, Loss: 168.13230895996094\n",
      "Iteration: 10350, Score avg: -20.54, Loss: 83.33708953857422\n",
      "Iteration: 10400, Score avg: -20.46, Loss: 191.06088256835938\n",
      "Iteration: 10450, Score avg: -20.72, Loss: 94.23818969726562\n",
      "Iteration: 10500, Score avg: -20.52, Loss: 73.24165344238281\n",
      "Iteration: 10550, Score avg: -20.48, Loss: 218.24972534179688\n",
      "Iteration: 10600, Score avg: -20.78, Loss: 88.64725494384766\n",
      "Iteration: 10650, Score avg: -20.62, Loss: 131.50132751464844\n",
      "Iteration: 10700, Score avg: -20.46, Loss: 165.9829864501953\n",
      "Iteration: 10750, Score avg: -20.36, Loss: 90.01404571533203\n",
      "Iteration: 10800, Score avg: -20.56, Loss: 130.86058044433594\n",
      "Iteration: 10850, Score avg: -20.66, Loss: 111.88626098632812\n",
      "Iteration: 10900, Score avg: -20.6, Loss: 94.27304077148438\n",
      "Iteration: 10950, Score avg: -20.6, Loss: 148.48216247558594\n",
      "Iteration: 11000, Score avg: -20.52, Loss: 99.00224304199219\n",
      "Iteration: 11050, Score avg: -20.66, Loss: 110.08296966552734\n",
      "Iteration: 11100, Score avg: -20.58, Loss: 60.244606018066406\n",
      "Iteration: 11150, Score avg: -20.48, Loss: 130.51077270507812\n",
      "Iteration: 11200, Score avg: -20.42, Loss: 101.87718963623047\n",
      "Iteration: 11250, Score avg: -20.56, Loss: 118.5966796875\n",
      "Iteration: 11300, Score avg: -20.64, Loss: 145.9760284423828\n",
      "Iteration: 11350, Score avg: -20.42, Loss: 224.40577697753906\n",
      "Iteration: 11400, Score avg: -20.54, Loss: 66.20067596435547\n",
      "Iteration: 11450, Score avg: -20.52, Loss: 114.60645294189453\n",
      "Iteration: 11500, Score avg: -20.44, Loss: 119.91210174560547\n",
      "Iteration: 11550, Score avg: -20.42, Loss: 103.94546508789062\n",
      "Iteration: 11600, Score avg: -20.42, Loss: 84.33831787109375\n",
      "Iteration: 11650, Score avg: -20.5, Loss: 169.71510314941406\n",
      "Iteration: 11700, Score avg: -20.58, Loss: 122.72300720214844\n",
      "Iteration: 11750, Score avg: -20.48, Loss: 93.50524139404297\n",
      "Iteration: 11800, Score avg: -20.36, Loss: 146.38107299804688\n",
      "Iteration: 11850, Score avg: -20.48, Loss: 121.67619323730469\n",
      "Iteration: 11900, Score avg: -20.5, Loss: 88.37849426269531\n",
      "Iteration: 11950, Score avg: -20.6, Loss: 96.88465881347656\n",
      "Iteration: 12000, Score avg: -20.42, Loss: 176.47885131835938\n",
      "Iteration: 12050, Score avg: -20.48, Loss: 130.3141632080078\n",
      "Iteration: 12100, Score avg: -20.62, Loss: 82.95553588867188\n",
      "Iteration: 12150, Score avg: -20.34, Loss: 186.1017608642578\n",
      "Iteration: 12200, Score avg: -20.6, Loss: 119.84099578857422\n",
      "Iteration: 12250, Score avg: -20.48, Loss: 131.30264282226562\n",
      "Iteration: 12300, Score avg: -20.56, Loss: 116.38679504394531\n",
      "Iteration: 12350, Score avg: -20.56, Loss: 120.43035888671875\n",
      "Iteration: 12400, Score avg: -20.56, Loss: 111.50222778320312\n",
      "Iteration: 12450, Score avg: -20.52, Loss: 93.65185546875\n",
      "Iteration: 12500, Score avg: -20.48, Loss: 177.20753479003906\n",
      "Iteration: 12550, Score avg: -20.58, Loss: 57.399688720703125\n",
      "Iteration: 12600, Score avg: -20.66, Loss: 147.74252319335938\n",
      "Iteration: 12650, Score avg: -20.42, Loss: 103.87101745605469\n",
      "Iteration: 12700, Score avg: -20.56, Loss: 79.21173858642578\n",
      "Iteration: 12750, Score avg: -20.58, Loss: 211.52261352539062\n",
      "Iteration: 12800, Score avg: -20.36, Loss: 86.80859375\n",
      "Iteration: 12850, Score avg: -20.5, Loss: 111.67826843261719\n",
      "Iteration: 12900, Score avg: -20.54, Loss: 118.03321075439453\n",
      "Iteration: 12950, Score avg: -20.4, Loss: 104.58241271972656\n",
      "Iteration: 13000, Score avg: -20.52, Loss: 97.06990051269531\n",
      "Iteration: 13050, Score avg: -20.46, Loss: 135.18148803710938\n",
      "Iteration: 13100, Score avg: -20.56, Loss: 163.2138214111328\n",
      "Iteration: 13150, Score avg: -20.54, Loss: 173.67410278320312\n",
      "Iteration: 13200, Score avg: -20.54, Loss: 134.64547729492188\n",
      "Iteration: 13250, Score avg: -20.38, Loss: 137.66941833496094\n",
      "Iteration: 13300, Score avg: -20.62, Loss: 119.83719635009766\n",
      "Iteration: 13350, Score avg: -20.52, Loss: 178.99427795410156\n",
      "Iteration: 13400, Score avg: -20.64, Loss: 105.68319702148438\n",
      "Iteration: 13450, Score avg: -20.38, Loss: 137.03306579589844\n",
      "Iteration: 13500, Score avg: -20.5, Loss: 159.34837341308594\n",
      "Iteration: 13550, Score avg: -20.48, Loss: 90.59119415283203\n",
      "Iteration: 13600, Score avg: -20.46, Loss: 167.85401916503906\n",
      "Iteration: 13650, Score avg: -20.4, Loss: 135.55526733398438\n",
      "Iteration: 13700, Score avg: -20.54, Loss: 175.45896911621094\n",
      "Iteration: 13750, Score avg: -20.6, Loss: 141.2670440673828\n",
      "Iteration: 13800, Score avg: -20.5, Loss: 113.65275573730469\n",
      "Iteration: 13850, Score avg: -20.42, Loss: 81.66204071044922\n",
      "Iteration: 13900, Score avg: -20.62, Loss: 49.762184143066406\n",
      "Iteration: 13950, Score avg: -20.42, Loss: 104.10298156738281\n",
      "Iteration: 14000, Score avg: -20.48, Loss: 220.32513427734375\n",
      "Iteration: 14050, Score avg: -20.72, Loss: 28.223237991333008\n",
      "Iteration: 14100, Score avg: -20.4, Loss: 179.1806182861328\n",
      "Iteration: 14150, Score avg: -20.6, Loss: 178.375\n",
      "Iteration: 14200, Score avg: -20.52, Loss: 88.88654327392578\n",
      "Iteration: 14250, Score avg: -20.42, Loss: 126.58322143554688\n",
      "Iteration: 14300, Score avg: -20.46, Loss: 93.026123046875\n",
      "Iteration: 14350, Score avg: -20.5, Loss: 75.87322998046875\n",
      "Iteration: 14400, Score avg: -20.34, Loss: 72.98119354248047\n",
      "Iteration: 14450, Score avg: -20.6, Loss: 98.9489974975586\n",
      "Iteration: 14500, Score avg: -20.48, Loss: 135.0255584716797\n",
      "Iteration: 14550, Score avg: -20.2, Loss: 165.70199584960938\n",
      "Iteration: 14600, Score avg: -20.72, Loss: 31.339353561401367\n",
      "Iteration: 14650, Score avg: -20.58, Loss: 101.17056274414062\n",
      "Iteration: 14700, Score avg: -20.56, Loss: 76.35871887207031\n",
      "Iteration: 14750, Score avg: -20.36, Loss: 98.7944107055664\n",
      "Iteration: 14800, Score avg: -20.54, Loss: 145.6009521484375\n",
      "Iteration: 14850, Score avg: -20.58, Loss: 142.27833557128906\n",
      "Iteration: 14900, Score avg: -20.44, Loss: 96.22554779052734\n",
      "Iteration: 14950, Score avg: -20.4, Loss: 124.70858764648438\n",
      "Iteration: 15000, Score avg: -20.38, Loss: 74.4919662475586\n",
      "Iteration: 15050, Score avg: -20.42, Loss: 227.85643005371094\n",
      "Iteration: 15100, Score avg: -20.46, Loss: 106.49671173095703\n",
      "Iteration: 15150, Score avg: -20.66, Loss: 131.87808227539062\n",
      "Iteration: 15200, Score avg: -20.56, Loss: 135.86155700683594\n",
      "Iteration: 15250, Score avg: -20.46, Loss: 66.11599731445312\n",
      "Iteration: 15300, Score avg: -20.06, Loss: 203.81285095214844\n",
      "Iteration: 15350, Score avg: -20.5, Loss: 118.0903549194336\n",
      "Iteration: 15400, Score avg: -20.5, Loss: 120.93791961669922\n",
      "Iteration: 15450, Score avg: -20.46, Loss: 85.8315658569336\n",
      "Iteration: 15500, Score avg: -20.24, Loss: 141.81549072265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 15550, Score avg: -20.38, Loss: 41.77320861816406\n",
      "Iteration: 15600, Score avg: -20.5, Loss: 124.51716613769531\n",
      "Iteration: 15650, Score avg: -20.36, Loss: 208.81201171875\n",
      "Iteration: 15700, Score avg: -20.4, Loss: 84.21087646484375\n",
      "Iteration: 15750, Score avg: -20.3, Loss: 154.0518798828125\n",
      "Iteration: 15800, Score avg: -20.3, Loss: 228.9610137939453\n",
      "Iteration: 15850, Score avg: -20.46, Loss: 157.6998291015625\n",
      "Iteration: 15900, Score avg: -20.26, Loss: 143.63604736328125\n",
      "Iteration: 15950, Score avg: -20.26, Loss: 153.16795349121094\n",
      "Iteration: 16000, Score avg: -20.36, Loss: 96.4077377319336\n",
      "Iteration: 16050, Score avg: -20.5, Loss: 128.59188842773438\n",
      "Iteration: 16100, Score avg: -20.4, Loss: 173.5692138671875\n",
      "Iteration: 16150, Score avg: -20.36, Loss: 199.22564697265625\n",
      "Iteration: 16200, Score avg: -20.58, Loss: 135.1899871826172\n",
      "Iteration: 16250, Score avg: -20.38, Loss: 152.04737854003906\n",
      "Iteration: 16300, Score avg: -20.3, Loss: 197.73941040039062\n",
      "Iteration: 16350, Score avg: -20.46, Loss: 132.1302032470703\n",
      "Iteration: 16400, Score avg: -20.36, Loss: 99.89250183105469\n",
      "Iteration: 16450, Score avg: -20.22, Loss: 127.37316131591797\n",
      "Iteration: 16500, Score avg: -20.3, Loss: 106.02909851074219\n",
      "Iteration: 16550, Score avg: -20.52, Loss: 40.34498977661133\n",
      "Iteration: 16600, Score avg: -20.18, Loss: 196.5950927734375\n",
      "Iteration: 16650, Score avg: -20.46, Loss: 217.5026397705078\n",
      "Iteration: 16700, Score avg: -20.26, Loss: 165.29171752929688\n",
      "Iteration: 16750, Score avg: -20.18, Loss: 195.94561767578125\n",
      "Iteration: 16800, Score avg: -20.34, Loss: 186.3057098388672\n",
      "Iteration: 16850, Score avg: -20.34, Loss: 146.33485412597656\n",
      "Iteration: 16900, Score avg: -20.54, Loss: 125.62674713134766\n",
      "Iteration: 16950, Score avg: -20.4, Loss: 152.31222534179688\n",
      "Iteration: 17000, Score avg: -20.3, Loss: 197.91729736328125\n",
      "Iteration: 17050, Score avg: -20.06, Loss: 204.89556884765625\n",
      "Iteration: 17100, Score avg: -20.06, Loss: 150.62489318847656\n",
      "Iteration: 17150, Score avg: -20.1, Loss: 125.03445434570312\n",
      "Iteration: 17200, Score avg: -20.32, Loss: 139.1860809326172\n",
      "Iteration: 17250, Score avg: -20.14, Loss: 164.04876708984375\n",
      "Iteration: 17300, Score avg: -19.96, Loss: 223.9595184326172\n",
      "Iteration: 17350, Score avg: -19.88, Loss: 222.2642822265625\n",
      "Iteration: 17400, Score avg: -20.22, Loss: 127.76261901855469\n",
      "Iteration: 17450, Score avg: -20.02, Loss: 208.92446899414062\n",
      "Iteration: 17500, Score avg: -20.38, Loss: 140.35733032226562\n",
      "Iteration: 17550, Score avg: -19.98, Loss: 240.01480102539062\n",
      "Iteration: 17600, Score avg: -20.22, Loss: 73.30880737304688\n",
      "Iteration: 17650, Score avg: -20.18, Loss: 89.39282989501953\n",
      "Iteration: 17700, Score avg: -20.3, Loss: 33.268272399902344\n",
      "Iteration: 17750, Score avg: -20.1, Loss: 148.0019989013672\n",
      "Iteration: 17800, Score avg: -20.18, Loss: 211.50576782226562\n",
      "Iteration: 17850, Score avg: -20.18, Loss: 161.35678100585938\n",
      "Iteration: 17900, Score avg: -20.02, Loss: 155.8920440673828\n",
      "Iteration: 17950, Score avg: -19.76, Loss: 172.2791748046875\n",
      "Iteration: 18000, Score avg: -19.82, Loss: 137.71376037597656\n",
      "Iteration: 18050, Score avg: -20.0, Loss: 118.2574234008789\n",
      "Iteration: 18100, Score avg: -20.0, Loss: 128.0675811767578\n",
      "Iteration: 18150, Score avg: -20.36, Loss: 158.59597778320312\n",
      "Iteration: 18200, Score avg: -19.9, Loss: 165.87086486816406\n",
      "Iteration: 18250, Score avg: -19.84, Loss: 139.31004333496094\n",
      "Iteration: 18300, Score avg: -19.82, Loss: 176.41943359375\n",
      "Iteration: 18350, Score avg: -20.04, Loss: 119.83108520507812\n",
      "Iteration: 18400, Score avg: -20.26, Loss: 114.10855102539062\n",
      "Iteration: 18450, Score avg: -20.08, Loss: 194.45677185058594\n",
      "Iteration: 18500, Score avg: -20.16, Loss: 274.8974609375\n",
      "Iteration: 18550, Score avg: -20.4, Loss: 122.38873291015625\n",
      "Iteration: 18600, Score avg: -19.94, Loss: 237.00942993164062\n",
      "Iteration: 18650, Score avg: -20.26, Loss: 91.46530151367188\n",
      "Iteration: 18700, Score avg: -19.98, Loss: 139.24224853515625\n",
      "Iteration: 18750, Score avg: -19.96, Loss: 245.63702392578125\n",
      "Iteration: 18800, Score avg: -20.02, Loss: 136.19435119628906\n",
      "Iteration: 18850, Score avg: -20.36, Loss: 127.0127944946289\n",
      "Iteration: 18900, Score avg: -20.1, Loss: 253.00546264648438\n",
      "Iteration: 18950, Score avg: -20.06, Loss: 172.86941528320312\n",
      "Iteration: 19000, Score avg: -20.08, Loss: 209.1334228515625\n",
      "Iteration: 19050, Score avg: -19.78, Loss: 156.54867553710938\n",
      "Iteration: 19100, Score avg: -20.04, Loss: 104.60062408447266\n",
      "Iteration: 19150, Score avg: -20.2, Loss: 199.23468017578125\n",
      "Iteration: 19200, Score avg: -20.04, Loss: 66.56636047363281\n",
      "Iteration: 19250, Score avg: -20.04, Loss: 188.00804138183594\n",
      "Iteration: 19300, Score avg: -19.82, Loss: 226.97906494140625\n",
      "Iteration: 19350, Score avg: -20.04, Loss: 205.4979248046875\n",
      "Iteration: 19400, Score avg: -20.02, Loss: 170.30844116210938\n",
      "Iteration: 19450, Score avg: -19.9, Loss: 221.6836395263672\n",
      "Iteration: 19500, Score avg: -19.98, Loss: 137.5163116455078\n",
      "Iteration: 19550, Score avg: -19.9, Loss: 132.83953857421875\n",
      "Iteration: 19600, Score avg: -19.7, Loss: 156.3408966064453\n",
      "Iteration: 19650, Score avg: -19.68, Loss: 78.6047134399414\n",
      "Iteration: 19700, Score avg: -19.84, Loss: 177.1161651611328\n",
      "Iteration: 19750, Score avg: -19.82, Loss: 188.7974853515625\n",
      "Iteration: 19800, Score avg: -20.12, Loss: 116.94849395751953\n",
      "Iteration: 19850, Score avg: -19.88, Loss: 218.2627410888672\n",
      "Iteration: 19900, Score avg: -19.84, Loss: 157.8397216796875\n",
      "Iteration: 19950, Score avg: -19.96, Loss: 218.59228515625\n",
      "Iteration: 20000, Score avg: -19.54, Loss: 123.1229019165039\n",
      "Iteration: 20050, Score avg: -19.82, Loss: 91.53948211669922\n",
      "Iteration: 20100, Score avg: -19.84, Loss: 139.42489624023438\n"
     ]
    }
   ],
   "source": [
    "learn_rate = 1e-4\n",
    "n_iters = 500000\n",
    "avg_over = 50\n",
    "val_coeff = 0.5\n",
    "ent_coeff = 0.01\n",
    "\n",
    "#init_screen = get_screen(env.reset())\n",
    "#_, num_frames, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "\n",
    "#actor_critic = ActorCritic(num_frames, n_actions).to(device)\n",
    "#critic = Critic(num_frames, screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "#optimizer = optim.Adam(actor_critic.parameters(), lr=learn_rate)\n",
    "#optimizerC = optim.Adam(critic.parameters(), lr=learn_rate)\n",
    "\n",
    "#score_avg = []\n",
    "#ac_loss_list = []\n",
    "#score_avg_all = []\n",
    "#ac_loss_list_all = []\n",
    "\n",
    "\n",
    "for iter in range(n_iters):\n",
    "    state = get_screen(env.reset())\n",
    "    #state = Variable(state)\n",
    "    #log_probs = []\n",
    "    steps = []\n",
    "    #values = []\n",
    "    #rewards = []\n",
    "    #masks = []\n",
    "    entropy = 0\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        #env.render()\n",
    "        #state = torch.FloatTensor(state).to(device)\n",
    "        logit, values = actor_critic(state)\n",
    "        \n",
    "        probs = F.softmax(logit)\n",
    "        actions = probs.multinomial(1).data\n",
    "        \n",
    "        state, rewards, dones, _ = env.step(actions.cpu().numpy())\n",
    "        reward_sum += rewards\n",
    "        #masks = (1. - torch.from_numpy(np.array(dones, dtype=np.float32)))\n",
    "        #rewards = torch.from_numpy(rewards)\n",
    "        #rewards = torch.from_numpy(np.array(rewards, dtype=np.float32))\n",
    "        rewards = (torch.tensor([rewards], dtype=torch.float, device=device)).unsqueeze(-1)\n",
    "        masks = (torch.tensor([1-dones], dtype=torch.float, device=device)).unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        state = get_screen(state)\n",
    "        #state = Variable(state)\n",
    "        \n",
    "        #reward_sum += rewards\n",
    "        \n",
    "        steps.append((rewards, masks, actions, logit, values))\n",
    "        if dones:\n",
    "            #print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "            score_avg.append(reward_sum)\n",
    "            reward_sum=0\n",
    "            break\n",
    "    \n",
    "        \n",
    "    \n",
    "    #next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, final_values = actor_critic(state)\n",
    "    steps.append((None, None, None, None, final_values))\n",
    "    actions, logit, values, returns, advantages = process_rollout()\n",
    "   \n",
    "    probs = F.softmax(logit)\n",
    "    log_probs = F.log_softmax(logit)\n",
    "    log_action_probs = log_probs.gather(1, actions)\n",
    "\n",
    "    \n",
    "    policy_loss = (-log_action_probs * advantages).sum()\n",
    "    value_loss = (0.5*(values - returns) ** 2.).sum()\n",
    "    entropy_loss = (log_probs * probs).sum()\n",
    "\n",
    "    ac_loss = policy_loss + value_loss * val_coeff + entropy_loss * ent_coeff\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    nn.utils.clip_grad_norm(actor_critic.parameters(), 40.)\n",
    "    optimizer.step()\n",
    "    \n",
    "    ac_loss_list.append(ac_loss)\n",
    "    #optimizer.zero_grad()\n",
    "    if iter % avg_over == 0 and iter != 0:\n",
    "        score_sum = 0\n",
    "        loss_sum = 0\n",
    "        for i in range(len(score_avg)):\n",
    "            score_sum += score_avg[i]\n",
    "            loss_sum += ac_loss_list[i]\n",
    "        s = score_sum/avg_over\n",
    "        l = loss_sum/avg_over\n",
    "        print('Iteration: {}, Score avg: {}, Loss: {}'.format(iter+8750, s, l))\n",
    "        score_avg_all.append(s)\n",
    "        ac_loss_list_all.append(l)\n",
    "        score_avg.clear()\n",
    "        ac_loss_list.clear()\n",
    "\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor_critic, 'results/actor.pkl')\n",
    "plt.plot(score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ac_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9f2cbb7bc6f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'done' is not defined"
     ]
    }
   ],
   "source": [
    "n_iter = 5\n",
    "for iter in range(n_iter):\n",
    "    state = get_screen(env.reset())\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        env.render()\n",
    "        logit, values = actor_critic(state)\n",
    "        \n",
    "        probs = F.softmax(logit)\n",
    "        actions = probs.multinomial(1).data\n",
    "        \n",
    "        state, rewards, dones, _ = env.step(actions.cpu().numpy())\n",
    "        reward_sum += rewards\n",
    "        masks = (1. - torch.from_numpy(np.array(dones, dtype=np.float32))).unsqueeze(-1)\n",
    "        #rewards = torch.from_numpy(rewards)\n",
    "        #rewards = torch.from_numpy(np.array(rewards, dtype=np.float32)).unsqueeze(-1)\n",
    "        #rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        #masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "        \n",
    "        \n",
    "        state = get_screen(state)\n",
    "        #state = Variable(state)\n",
    "        \n",
    "        #reward_sum += rewards\n",
    "        \n",
    "        steps.append((rewards, masks, actions, logit, values))\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor, 'results/actor.pkl')\n",
    "torch.save(critic, 'results/critic.pkl')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
