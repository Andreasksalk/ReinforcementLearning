{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4)\n",
      "6\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gym, os\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Multinomial\n",
    "import torchvision.transforms as T\n",
    "from wrappers import *\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = make_env(env)\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "print(env.observation_space.shape)\n",
    "action_space = env.action_space.n\n",
    "print(action_space)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 3000\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def process_rollout(gamma = 0.99, lambd = 1.0, num_workers = 1):\n",
    "    _, _, _, _, last_values = steps[-1]\n",
    "    returns = last_values.data\n",
    "\n",
    "    advantages = torch.zeros(num_workers, 1)\n",
    "    #if cuda: advantages = advantages.cuda()\n",
    "\n",
    "    out = [None] * (len(steps) - 1)\n",
    "\n",
    "    # run Generalized Advantage Estimation, calculate returns, advantages\n",
    "    for t in reversed(range(len(steps) - 1)):\n",
    "        rewards, masks, actions, policies, values = steps[t]\n",
    "        _, _, _, _, next_values = steps[t + 1]\n",
    "\n",
    "        returns = rewards + returns * gamma * masks\n",
    "\n",
    "        deltas = rewards + next_values.data * gamma * masks - values.data\n",
    "        advantages = advantages * gamma * lambd * masks + deltas\n",
    "\n",
    "        out[t] = actions, policies, values, returns, advantages\n",
    "\n",
    "    # return data as batched Tensors, Variables\n",
    "    return map(lambda x: torch.cat(x, 0), zip(*out))\n",
    "\n",
    "def get_screen(x):\n",
    "    state = np.array(x)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    state = torch.from_numpy(state)\n",
    "    state = state.float() / 255\n",
    "    return state.unsqueeze(0)\n",
    "\n",
    "def ortho_weights(shape, scale=1.):\n",
    "    \"\"\" PyTorch port of ortho_init from baselines.a2c.utils \"\"\"\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        flat_shape = shape[1], shape[0]\n",
    "    elif len(shape) == 4:\n",
    "        flat_shape = (np.prod(shape[1:]), shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    a = np.random.normal(0., 1., flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.transpose().copy().reshape(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        return torch.from_numpy((scale * q).astype(np.float32))\n",
    "    if len(shape) == 4:\n",
    "        return torch.from_numpy((scale * q[:, :shape[1], :shape[2]]).astype(np.float32))\n",
    "\n",
    "def game_initializer(module):\n",
    "    \"\"\" Parameter initializer for Atari models\n",
    "    Initializes Linear, Conv2d, and LSTM weights.\n",
    "    \"\"\"\n",
    "    classname = module.__class__.__name__\n",
    "\n",
    "    if classname == 'Linear':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'Conv2d':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'LSTM':\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'weight_hh' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'bias' in name:\n",
    "                param.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two seperate networks the actor and the critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_out = 64\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_frames, h, w, outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames, 32, kernel_size=8, stride=4)\n",
    "        # self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, end_out, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.down = nn.Linear(7*7*64, 512)\n",
    "        self.a = nn.Linear(512, outputs)\n",
    "        self.c = nn.Linear(512, 1)\n",
    "        \n",
    "        self.apply(game_initializer)\n",
    "        self.a.weight.data = ortho_weights(self.a.weight.size(), scale=.01)\n",
    "        self.c.weight.data = ortho_weights(self.c.weight.size())\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.down(x.view(x.size(0), -1))\n",
    "        a = self.a(x)\n",
    "        c = self.c(x)\n",
    "        return a, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Score avg: -21.0, Loss: -2473.2734375, A: -3804.576171875, C: 2662.60546875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-014b27202330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mac_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m#optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_rate = 1e-5\n",
    "n_iters = 5000\n",
    "avg_over = 1\n",
    "val_coeff = 0.5\n",
    "ent_coeff = 0.01\n",
    "\n",
    "init_screen = get_screen(env.reset())\n",
    "_, num_frames, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "actor_critic = ActorCritic(num_frames, screen_height, screen_width, n_actions).to(device)\n",
    "#critic = Critic(num_frames, screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "optimizer = optim.Adam(actor_critic.parameters(), lr=learn_rate)\n",
    "#optimizerC = optim.Adam(critic.parameters(), lr=learn_rate)\n",
    "\n",
    "score_avg = []\n",
    "ac_loss_list = []\n",
    "\n",
    "\n",
    "for iter in range(n_iters):\n",
    "    state = get_screen(env.reset())\n",
    "    #state = Variable(state)\n",
    "    log_probs = []\n",
    "    steps = []\n",
    "    values = []\n",
    "    #rewards = []\n",
    "    #masks = []\n",
    "    entropy = 0\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        #env.render()\n",
    "        #state = torch.FloatTensor(state).to(device)\n",
    "        logit, values = actor_critic(state)\n",
    "        \n",
    "        probs = F.softmax(logit)\n",
    "        dist = Categorical(probs)\n",
    "        actions = dist.sample()\n",
    "        \n",
    "        state, rewards, dones, _ = env.step(actions.cpu().numpy())\n",
    "        reward_sum += rewards\n",
    "        masks = (1. - torch.from_numpy(np.array(dones, dtype=np.float32))).unsqueeze(-1)\n",
    "        rewards = torch.from_numpy(np.array(rewards, dtype=np.float32)).unsqueeze(-1)\n",
    "        #rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        #masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "        \n",
    "        \n",
    "        state = get_screen(state)\n",
    "        #state = Variable(state)\n",
    "        \n",
    "        #reward_sum += rewards\n",
    "        \n",
    "        steps.append((rewards, masks, actions, logit, values))\n",
    "        if dones:\n",
    "            #print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "            score_avg.append(reward_sum)\n",
    "            reward_sum=0\n",
    "            break\n",
    "    \n",
    "        \n",
    "    \n",
    "    #next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, final_values = actor_critic(state)\n",
    "    steps.append((None, None, None, None, final_values))\n",
    "    actions, logit, values, returns, advantages = process_rollout()\n",
    "   \n",
    "    probs = F.softmax(logit)\n",
    "    log_probs = F.log_softmax(logit)\n",
    "    log_action_probs = log_probs.gather(1, actions.unsqueeze(1))\n",
    "\n",
    "    \n",
    "    policy_loss = (-log_action_probs * advantages).sum()\n",
    "    value_loss = (0.5*(values - returns) ** 2.).sum()\n",
    "    entropy_loss = (log_probs * probs).mean()\n",
    "\n",
    "    ac_loss = policy_loss + value_loss * val_coeff #+ entropy_loss * ent_coeff\n",
    "    print('Iteration: {}, Score avg: {}, Loss: {}, A: {}, C: {}'.format(iter, score_avg[iter], ac_loss, policy_loss, value_loss))\n",
    "    \n",
    "    \n",
    "    ac_loss_list.append(ac_loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    optimizer.step()\n",
    "    #optimizer.zero_grad()\n",
    "    steps = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ac_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-761ee3a37bef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "n_iter = 5\n",
    "for iter in range(n_iter):\n",
    "    state = get_screen(env.reset())\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        env.render()\n",
    "        #state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = actor_critic(state)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        next_state = get_screen(next_state)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor, 'results/actor.pkl')\n",
    "torch.save(critic, 'results/critic.pkl')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
