{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4)\n",
      "6\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gym, os\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Multinomial\n",
    "import torchvision.transforms as T\n",
    "from wrappers import *\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "import statistics\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"Pong-v0\")\n",
    "env = make_env(env)\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "print(env.observation_space.shape)\n",
    "action_space = env.action_space.n\n",
    "print(action_space)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 3000\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def process_rollout(gamma = 0.99, lambd = 1.0, num_workers = 1):\n",
    "    _, _, _, _, last_values = steps[-1]\n",
    "    returns = last_values.data\n",
    "\n",
    "    advantages = torch.zeros(num_workers, 1)\n",
    "    #if cuda: advantages = advantages.cuda()\n",
    "\n",
    "    out = [None] * (len(steps) - 1)\n",
    "\n",
    "    # run Generalized Advantage Estimation, calculate returns, advantages\n",
    "    for t in reversed(range(len(steps) - 1)):\n",
    "        rewards, masks, actions, policies, values = steps[t]\n",
    "        _, _, _, _, next_values = steps[t + 1]\n",
    "\n",
    "        returns = rewards + returns * gamma * masks\n",
    "\n",
    "        deltas = rewards + next_values.data * gamma * masks - values.data\n",
    "        advantages = advantages * gamma * lambd * masks + deltas\n",
    "\n",
    "        out[t] = actions, policies, values, returns, advantages\n",
    "\n",
    "    # return data as batched Tensors, Variables\n",
    "    return map(lambda x: torch.cat(x, 0), zip(*out))\n",
    "\n",
    "def get_screen(x):\n",
    "    state = np.array(x)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    state = torch.from_numpy(state)\n",
    "    state = state.float() / 255\n",
    "    return state.unsqueeze(0)\n",
    "\n",
    "def ortho_weights(shape, scale=1.):\n",
    "    \"\"\" PyTorch port of ortho_init from baselines.a2c.utils \"\"\"\n",
    "    shape = tuple(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        flat_shape = shape[1], shape[0]\n",
    "    elif len(shape) == 4:\n",
    "        flat_shape = (np.prod(shape[1:]), shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    a = np.random.normal(0., 1., flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.transpose().copy().reshape(shape)\n",
    "\n",
    "    if len(shape) == 2:\n",
    "        return torch.from_numpy((scale * q).astype(np.float32))\n",
    "    if len(shape) == 4:\n",
    "        return torch.from_numpy((scale * q[:, :shape[1], :shape[2]]).astype(np.float32))\n",
    "\n",
    "def game_initializer(module):\n",
    "    \"\"\" Parameter initializer for Atari models\n",
    "    Initializes Linear, Conv2d, and LSTM weights.\n",
    "    \"\"\"\n",
    "    classname = module.__class__.__name__\n",
    "\n",
    "    if classname == 'Linear':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'Conv2d':\n",
    "        module.weight.data = ortho_weights(module.weight.data.size(), scale=np.sqrt(2.))\n",
    "        module.bias.data.zero_()\n",
    "\n",
    "    elif classname == 'LSTM':\n",
    "        for name, param in module.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'weight_hh' in name:\n",
    "                param.data = ortho_weights(param.data.size(), scale=1.)\n",
    "            if 'bias' in name:\n",
    "                param.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two seperate networks the actor and the critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_out = 64\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_frames, outputs):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames, 32, kernel_size=8, stride=4)\n",
    "        # self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        # self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, end_out, kernel_size=3, stride=1)\n",
    "        \n",
    "        self.down = nn.Linear(7*7*64, 512)\n",
    "        self.a = nn.Linear(512, outputs)\n",
    "        self.c = nn.Linear(512, 1)\n",
    "        \n",
    "        self.apply(game_initializer)\n",
    "        self.a.weight.data = ortho_weights(self.a.weight.size(), scale=.01)\n",
    "        self.c.weight.data = ortho_weights(self.c.weight.size())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.down(x.view(x.size(0), -1))\n",
    "        a = self.a(x)\n",
    "        c = self.c(x)\n",
    "        return a, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:72: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:85: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50, Score avg: -20.96, Loss: 282.3039855957031\n",
      "Iteration: 100, Score avg: -20.38, Loss: 467.82391357421875\n",
      "Iteration: 150, Score avg: -20.42, Loss: 394.04461669921875\n",
      "Iteration: 200, Score avg: -20.52, Loss: 304.4825439453125\n",
      "Iteration: 250, Score avg: -20.38, Loss: 238.1055145263672\n",
      "Iteration: 300, Score avg: -20.42, Loss: 227.94454956054688\n",
      "Iteration: 350, Score avg: -20.32, Loss: 196.4358367919922\n",
      "Iteration: 400, Score avg: -20.52, Loss: 252.54867553710938\n",
      "Iteration: 450, Score avg: -20.4, Loss: 182.66552734375\n",
      "Iteration: 500, Score avg: -20.44, Loss: 201.83441162109375\n",
      "Iteration: 550, Score avg: -20.36, Loss: 238.1215362548828\n",
      "Iteration: 600, Score avg: -20.5, Loss: 142.74374389648438\n",
      "Iteration: 650, Score avg: -20.56, Loss: 59.531829833984375\n",
      "Iteration: 700, Score avg: -20.42, Loss: 140.17816162109375\n",
      "Iteration: 750, Score avg: -20.44, Loss: 184.8663330078125\n",
      "Iteration: 800, Score avg: -20.58, Loss: 253.32696533203125\n",
      "Iteration: 850, Score avg: -20.36, Loss: 184.8706817626953\n",
      "Iteration: 900, Score avg: -20.48, Loss: 69.8950424194336\n",
      "Iteration: 950, Score avg: -20.48, Loss: 131.83941650390625\n",
      "Iteration: 1000, Score avg: -20.58, Loss: 140.01832580566406\n",
      "Iteration: 1050, Score avg: -20.62, Loss: 95.20943450927734\n",
      "Iteration: 1100, Score avg: -20.54, Loss: 138.3782958984375\n",
      "Iteration: 1150, Score avg: -20.64, Loss: 147.22178649902344\n",
      "Iteration: 1200, Score avg: -20.38, Loss: 167.54412841796875\n",
      "Iteration: 1250, Score avg: -20.54, Loss: 184.79673767089844\n",
      "Iteration: 1300, Score avg: -20.46, Loss: 44.45082092285156\n",
      "Iteration: 1350, Score avg: -20.7, Loss: 87.50453186035156\n",
      "Iteration: 1400, Score avg: -20.56, Loss: 151.64291381835938\n",
      "Iteration: 1450, Score avg: -20.62, Loss: 179.5986785888672\n",
      "Iteration: 1500, Score avg: -20.7, Loss: 104.50335693359375\n",
      "Iteration: 1550, Score avg: -20.44, Loss: 103.82415771484375\n",
      "Iteration: 1600, Score avg: -20.48, Loss: 112.50823211669922\n",
      "Iteration: 1650, Score avg: -20.4, Loss: 100.892578125\n",
      "Iteration: 1700, Score avg: -20.38, Loss: 127.8171615600586\n",
      "Iteration: 1750, Score avg: -20.7, Loss: 97.26814270019531\n",
      "Iteration: 1800, Score avg: -20.66, Loss: 69.22880554199219\n",
      "Iteration: 1850, Score avg: -20.56, Loss: 169.9092254638672\n",
      "Iteration: 1900, Score avg: -20.56, Loss: 88.7958984375\n",
      "Iteration: 1950, Score avg: -20.36, Loss: 47.8062744140625\n",
      "Iteration: 2000, Score avg: -20.48, Loss: 79.53064727783203\n",
      "Iteration: 2050, Score avg: -20.52, Loss: 85.40230560302734\n",
      "Iteration: 2100, Score avg: -20.64, Loss: 178.80755615234375\n",
      "Iteration: 2150, Score avg: -20.54, Loss: 217.97576904296875\n",
      "Iteration: 2200, Score avg: -20.26, Loss: 134.1873779296875\n",
      "Iteration: 2250, Score avg: -20.54, Loss: 99.33397674560547\n",
      "Iteration: 2300, Score avg: -20.62, Loss: 115.81434631347656\n",
      "Iteration: 2350, Score avg: -20.56, Loss: 87.79808807373047\n",
      "Iteration: 2400, Score avg: -20.44, Loss: 98.3360824584961\n",
      "Iteration: 2450, Score avg: -20.38, Loss: 140.86224365234375\n",
      "Iteration: 2500, Score avg: -20.4, Loss: 149.19467163085938\n",
      "Iteration: 2550, Score avg: -20.56, Loss: 101.41007995605469\n",
      "Iteration: 2600, Score avg: -20.56, Loss: 150.63880920410156\n",
      "Iteration: 2650, Score avg: -20.7, Loss: 80.14142608642578\n",
      "Iteration: 2700, Score avg: -20.48, Loss: 59.572120666503906\n",
      "Iteration: 2750, Score avg: -20.48, Loss: 198.6449432373047\n",
      "Iteration: 2800, Score avg: -20.28, Loss: 67.29437255859375\n",
      "Iteration: 2850, Score avg: -20.58, Loss: 59.614158630371094\n",
      "Iteration: 2900, Score avg: -20.68, Loss: 113.55268859863281\n",
      "Iteration: 2950, Score avg: -20.56, Loss: 113.08984375\n",
      "Iteration: 3000, Score avg: -20.62, Loss: 116.67298889160156\n",
      "Iteration: 3050, Score avg: -20.38, Loss: 134.12789916992188\n",
      "Iteration: 3100, Score avg: -20.6, Loss: 156.7850799560547\n",
      "Iteration: 3150, Score avg: -20.54, Loss: 110.26383972167969\n",
      "Iteration: 3200, Score avg: -20.64, Loss: 71.8153305053711\n",
      "Iteration: 3250, Score avg: -20.48, Loss: 133.135009765625\n",
      "Iteration: 3300, Score avg: -20.4, Loss: 141.04531860351562\n",
      "Iteration: 3350, Score avg: -20.42, Loss: 68.69831085205078\n",
      "Iteration: 3400, Score avg: -20.46, Loss: 168.27685546875\n",
      "Iteration: 3450, Score avg: -20.62, Loss: 113.91895294189453\n",
      "Iteration: 3500, Score avg: -20.58, Loss: 88.89546203613281\n",
      "Iteration: 3550, Score avg: -20.56, Loss: 63.55647277832031\n",
      "Iteration: 3600, Score avg: -20.62, Loss: 149.47068786621094\n",
      "Iteration: 3650, Score avg: -20.56, Loss: 95.80113220214844\n",
      "Iteration: 3700, Score avg: -20.34, Loss: 107.51606750488281\n",
      "Iteration: 3750, Score avg: -20.48, Loss: 89.37808227539062\n",
      "Iteration: 3800, Score avg: -20.46, Loss: 112.15434265136719\n",
      "Iteration: 3850, Score avg: -20.66, Loss: 91.18143463134766\n",
      "Iteration: 3900, Score avg: -20.52, Loss: 95.95751953125\n",
      "Iteration: 3950, Score avg: -20.36, Loss: 89.48392486572266\n",
      "Iteration: 4000, Score avg: -20.66, Loss: 132.6106414794922\n",
      "Iteration: 4050, Score avg: -20.62, Loss: 104.44255828857422\n",
      "Iteration: 4100, Score avg: -20.64, Loss: 135.73008728027344\n",
      "Iteration: 4150, Score avg: -20.48, Loss: 180.57728576660156\n",
      "Iteration: 4200, Score avg: -20.58, Loss: 106.55162048339844\n",
      "Iteration: 4250, Score avg: -20.44, Loss: 107.09077453613281\n",
      "Iteration: 4300, Score avg: -20.72, Loss: 112.63166809082031\n",
      "Iteration: 4350, Score avg: -20.52, Loss: 142.85719299316406\n",
      "Iteration: 4400, Score avg: -20.62, Loss: 80.60340118408203\n",
      "Iteration: 4450, Score avg: -20.62, Loss: 162.05242919921875\n",
      "Iteration: 4500, Score avg: -20.34, Loss: 144.420654296875\n",
      "Iteration: 4550, Score avg: -20.42, Loss: 65.66004180908203\n",
      "Iteration: 4600, Score avg: -20.4, Loss: 140.8903350830078\n",
      "Iteration: 4650, Score avg: -20.54, Loss: 105.37943267822266\n",
      "Iteration: 4700, Score avg: -20.62, Loss: 27.336597442626953\n",
      "Iteration: 4750, Score avg: -20.5, Loss: 102.18335723876953\n",
      "Iteration: 4800, Score avg: -20.48, Loss: 171.47775268554688\n",
      "Iteration: 4850, Score avg: -20.62, Loss: 86.03881072998047\n",
      "Iteration: 4900, Score avg: -20.5, Loss: 128.82354736328125\n",
      "Iteration: 4950, Score avg: -20.68, Loss: 114.32982635498047\n",
      "Iteration: 5000, Score avg: -20.52, Loss: 57.14223098754883\n",
      "Iteration: 5050, Score avg: -20.7, Loss: 149.15679931640625\n",
      "Iteration: 5100, Score avg: -20.44, Loss: 140.43283081054688\n",
      "Iteration: 5150, Score avg: -20.4, Loss: 63.11711883544922\n",
      "Iteration: 5200, Score avg: -20.38, Loss: 105.603759765625\n",
      "Iteration: 5250, Score avg: -20.64, Loss: 78.10633850097656\n",
      "Iteration: 5300, Score avg: -20.64, Loss: 104.00248718261719\n",
      "Iteration: 5350, Score avg: -20.6, Loss: 102.66952514648438\n",
      "Iteration: 5400, Score avg: -20.58, Loss: 138.6631317138672\n",
      "Iteration: 5450, Score avg: -20.56, Loss: 80.8016586303711\n",
      "Iteration: 5500, Score avg: -20.5, Loss: 183.01759338378906\n",
      "Iteration: 5550, Score avg: -20.5, Loss: 103.53900146484375\n",
      "Iteration: 5600, Score avg: -20.34, Loss: 158.54600524902344\n",
      "Iteration: 5650, Score avg: -20.6, Loss: 76.49683380126953\n",
      "Iteration: 5700, Score avg: -20.62, Loss: 139.11961364746094\n",
      "Iteration: 5750, Score avg: -20.5, Loss: 163.45188903808594\n",
      "Iteration: 5800, Score avg: -20.58, Loss: 163.92315673828125\n",
      "Iteration: 5850, Score avg: -20.54, Loss: 166.48179626464844\n",
      "Iteration: 5900, Score avg: -20.46, Loss: 155.11773681640625\n",
      "Iteration: 5950, Score avg: -20.44, Loss: 175.6454620361328\n",
      "Iteration: 6000, Score avg: -20.5, Loss: 129.40606689453125\n",
      "Iteration: 6050, Score avg: -20.42, Loss: 92.35507202148438\n",
      "Iteration: 6100, Score avg: -20.74, Loss: 120.08768463134766\n",
      "Iteration: 6150, Score avg: -20.34, Loss: 92.6020278930664\n",
      "Iteration: 6200, Score avg: -20.66, Loss: 131.02578735351562\n",
      "Iteration: 6250, Score avg: -20.6, Loss: 68.75552368164062\n",
      "Iteration: 6300, Score avg: -20.5, Loss: 75.58917999267578\n",
      "Iteration: 6350, Score avg: -20.56, Loss: 153.19810485839844\n",
      "Iteration: 6400, Score avg: -20.6, Loss: 139.01675415039062\n",
      "Iteration: 6450, Score avg: -20.54, Loss: 149.4874267578125\n",
      "Iteration: 6500, Score avg: -20.58, Loss: 79.8416519165039\n",
      "Iteration: 6550, Score avg: -20.6, Loss: 129.90821838378906\n",
      "Iteration: 6600, Score avg: -20.56, Loss: 87.56044006347656\n",
      "Iteration: 6650, Score avg: -20.54, Loss: 111.66841125488281\n",
      "Iteration: 6700, Score avg: -20.64, Loss: 127.7245864868164\n",
      "Iteration: 6750, Score avg: -20.58, Loss: 176.35079956054688\n",
      "Iteration: 6800, Score avg: -20.52, Loss: 130.99765014648438\n",
      "Iteration: 6850, Score avg: -20.24, Loss: 132.24591064453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6900, Score avg: -20.64, Loss: 123.48957061767578\n",
      "Iteration: 6950, Score avg: -20.58, Loss: 165.39682006835938\n",
      "Iteration: 7000, Score avg: -20.44, Loss: 136.24737548828125\n",
      "Iteration: 7050, Score avg: -20.52, Loss: 137.9613800048828\n",
      "Iteration: 7100, Score avg: -20.6, Loss: 58.385353088378906\n",
      "Iteration: 7150, Score avg: -20.54, Loss: 113.27423858642578\n",
      "Iteration: 7200, Score avg: -20.56, Loss: 124.04861450195312\n",
      "Iteration: 7250, Score avg: -20.54, Loss: 68.62422180175781\n",
      "Iteration: 7300, Score avg: -20.34, Loss: 133.56907653808594\n",
      "Iteration: 7350, Score avg: -20.4, Loss: 173.4021453857422\n",
      "Iteration: 7400, Score avg: -20.52, Loss: 117.35369873046875\n",
      "Iteration: 7450, Score avg: -20.42, Loss: 141.75218200683594\n",
      "Iteration: 7500, Score avg: -20.54, Loss: 143.6628875732422\n",
      "Iteration: 7550, Score avg: -20.62, Loss: 134.37644958496094\n",
      "Iteration: 7600, Score avg: -20.4, Loss: 195.7388458251953\n",
      "Iteration: 7650, Score avg: -20.62, Loss: 115.85310363769531\n",
      "Iteration: 7700, Score avg: -20.56, Loss: 156.02122497558594\n",
      "Iteration: 7750, Score avg: -20.6, Loss: 177.49093627929688\n",
      "Iteration: 7800, Score avg: -20.44, Loss: 248.85226440429688\n",
      "Iteration: 7850, Score avg: -20.54, Loss: 147.924072265625\n",
      "Iteration: 7900, Score avg: -20.5, Loss: 75.5213851928711\n",
      "Iteration: 7950, Score avg: -20.6, Loss: 136.95957946777344\n",
      "Iteration: 8000, Score avg: -20.62, Loss: 173.53387451171875\n",
      "Iteration: 8050, Score avg: -20.6, Loss: 126.39247131347656\n",
      "Iteration: 8100, Score avg: -20.6, Loss: 121.77022552490234\n",
      "Iteration: 8150, Score avg: -20.46, Loss: 139.5724334716797\n",
      "Iteration: 8200, Score avg: -20.52, Loss: 74.28775024414062\n",
      "Iteration: 8250, Score avg: -20.66, Loss: 68.93380737304688\n",
      "Iteration: 8300, Score avg: -20.44, Loss: 132.22000122070312\n",
      "Iteration: 8350, Score avg: -20.56, Loss: 132.10260009765625\n",
      "Iteration: 8400, Score avg: -20.58, Loss: 76.36283111572266\n",
      "Iteration: 8450, Score avg: -20.56, Loss: 106.57941436767578\n",
      "Iteration: 8500, Score avg: -20.3, Loss: 117.42494201660156\n",
      "Iteration: 8550, Score avg: -20.6, Loss: 128.4772491455078\n",
      "Iteration: 8600, Score avg: -20.4, Loss: 127.52396392822266\n",
      "Iteration: 8650, Score avg: -20.44, Loss: 109.90642547607422\n",
      "Iteration: 8700, Score avg: -20.48, Loss: 95.63687133789062\n",
      "Iteration: 8750, Score avg: -20.4, Loss: 61.721004486083984\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d4ba9c0ed368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#masks = (1. - torch.from_numpy(np.array(dones, dtype=np.float32)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ReinforcementLearning/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ReinforcementLearning/wrappers.py\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_rate = 1e-4\n",
    "n_iters = 500000\n",
    "avg_over = 50\n",
    "val_coeff = 0.5\n",
    "ent_coeff = 0.01\n",
    "\n",
    "#init_screen = get_screen(env.reset())\n",
    "#_, num_frames, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "\n",
    "#actor_critic = ActorCritic(num_frames, n_actions).to(device)\n",
    "#critic = Critic(num_frames, screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "#optimizer = optim.Adam(actor_critic.parameters(), lr=learn_rate)\n",
    "#optimizerC = optim.Adam(critic.parameters(), lr=learn_rate)\n",
    "\n",
    "#score_avg = []\n",
    "#ac_loss_list = []\n",
    "#score_avg_all = []\n",
    "#ac_loss_list_all = []\n",
    "\n",
    "\n",
    "for iter in range(n_iters):\n",
    "    state = get_screen(env.reset())\n",
    "    #state = Variable(state)\n",
    "    #log_probs = []\n",
    "    steps = []\n",
    "    #values = []\n",
    "    #rewards = []\n",
    "    #masks = []\n",
    "    entropy = 0\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        #env.render()\n",
    "        #state = torch.FloatTensor(state).to(device)\n",
    "        logit, values = actor_critic(state)\n",
    "        \n",
    "        probs = F.softmax(logit)\n",
    "        actions = probs.multinomial(1).data\n",
    "        \n",
    "        state, rewards, dones, _ = env.step(actions.cpu().numpy())\n",
    "        reward_sum += rewards\n",
    "        #masks = (1. - torch.from_numpy(np.array(dones, dtype=np.float32)))\n",
    "        #rewards = torch.from_numpy(rewards)\n",
    "        #rewards = torch.from_numpy(np.array(rewards, dtype=np.float32))\n",
    "        rewards = (torch.tensor([rewards], dtype=torch.float, device=device)).unsqueeze(-1)\n",
    "        masks = (torch.tensor([1-dones], dtype=torch.float, device=device)).unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        state = get_screen(state)\n",
    "        #state = Variable(state)\n",
    "        \n",
    "        #reward_sum += rewards\n",
    "        \n",
    "        steps.append((rewards, masks, actions, logit, values))\n",
    "        if dones:\n",
    "            #print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "            score_avg.append(reward_sum)\n",
    "            reward_sum=0\n",
    "            break\n",
    "    \n",
    "        \n",
    "    \n",
    "    #next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, final_values = actor_critic(state)\n",
    "    steps.append((None, None, None, None, final_values))\n",
    "    actions, logit, values, returns, advantages = process_rollout()\n",
    "   \n",
    "    probs = F.softmax(logit)\n",
    "    log_probs = F.log_softmax(logit)\n",
    "    log_action_probs = log_probs.gather(1, actions)\n",
    "\n",
    "    \n",
    "    policy_loss = (-log_action_probs * advantages).sum()\n",
    "    value_loss = (0.5*(values - returns) ** 2.).sum()\n",
    "    entropy_loss = (log_probs * probs).sum()\n",
    "\n",
    "    ac_loss = policy_loss + value_loss * val_coeff + entropy_loss * ent_coeff\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    ac_loss.backward()\n",
    "    nn.utils.clip_grad_norm(actor_critic.parameters(), 40.)\n",
    "    optimizer.step()\n",
    "    \n",
    "    ac_loss_list.append(ac_loss)\n",
    "    #optimizer.zero_grad()\n",
    "    if iter % avg_over == 0 and iter != 0:\n",
    "        score_sum = 0\n",
    "        loss_sum = 0\n",
    "        for i in range(len(score_avg)):\n",
    "            score_sum += score_avg[i]\n",
    "            loss_sum += ac_loss_list[i]\n",
    "        s = score_sum/avg_over\n",
    "        l = loss_sum/avg_over\n",
    "        print('Iteration: {}, Score avg: {}, Loss: {}'.format(iter+8750, s, l))\n",
    "        score_avg_all.append(s)\n",
    "        ac_loss_list_all.append(l)\n",
    "        score_avg.clear()\n",
    "        ac_loss_list.clear()\n",
    "\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor_critic, 'results/actor.pkl')\n",
    "plt.plot(score_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ac_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9f2cbb7bc6f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'done' is not defined"
     ]
    }
   ],
   "source": [
    "n_iter = 5\n",
    "for iter in range(n_iter):\n",
    "    state = get_screen(env.reset())\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        env.render()\n",
    "        logit, values = actor_critic(state)\n",
    "        \n",
    "        probs = F.softmax(logit)\n",
    "        actions = probs.multinomial(1).data\n",
    "        \n",
    "        state, rewards, dones, _ = env.step(actions.cpu().numpy())\n",
    "        reward_sum += rewards\n",
    "        masks = (1. - torch.from_numpy(np.array(dones, dtype=np.float32))).unsqueeze(-1)\n",
    "        #rewards = torch.from_numpy(rewards)\n",
    "        #rewards = torch.from_numpy(np.array(rewards, dtype=np.float32)).unsqueeze(-1)\n",
    "        #rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        #masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "        \n",
    "        \n",
    "        state = get_screen(state)\n",
    "        #state = Variable(state)\n",
    "        \n",
    "        #reward_sum += rewards\n",
    "        \n",
    "        steps.append((rewards, masks, actions, logit, values))\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor, 'results/actor.pkl')\n",
    "torch.save(critic, 'results/critic.pkl')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
