{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n",
      "6\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import gym, os\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "state_space = env.observation_space.shape[0]\n",
    "print(env.observation_space.shape)\n",
    "action_space = env.action_space.n\n",
    "print(action_space)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 1000000\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def get_screen(x):\n",
    "    x = np.array(x)\n",
    "    screen = x.transpose((2,0,1)) #(height,width,num_frames) -> (num_frames,heigth,width)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two seperate networks the actor and the critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 480\n",
    "num_frames = 3\n",
    "end_out = 2\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(Actor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames, 4, kernel_size=5, stride=2)\n",
    "        #self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.conv2 = nn.Conv2d(4, 2, kernel_size=5, stride=2)\n",
    "        #self.bn2 = nn.BatchNorm2d(4)\n",
    "        self.conv3 = nn.Conv2d(2, end_out, kernel_size=5, stride=2)\n",
    "        #self.bn3 = nn.BatchNorm2d(end_out)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * end_out\n",
    "        self.out = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        #x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.out(x.view(x.size(0), -1))\n",
    "        \n",
    "        x = Categorical(F.softmax(x, dim=-1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(Critic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_frames, 4, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.conv2 = nn.Conv2d(4, 2, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(2)\n",
    "        self.conv3 = nn.Conv2d(2, end_out, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(end_out)\n",
    "        \n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * end_out\n",
    "        self.out = nn.Linear(linear_input_size, 1)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.out(x.view(x.size(0), -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5, Score avg: -26.666666666666668, Actor loss: -3.666351318359375, Critic: 3.7805747985839844\n",
      "Iteration: 10, Score avg: -27.333333333333332, Actor loss: -3.614302635192871, Critic: 3.5978128910064697\n",
      "Iteration: 15, Score avg: -26.666666666666668, Actor loss: -3.2346508502960205, Critic: 3.111171007156372\n",
      "Iteration: 20, Score avg: -27.666666666666668, Actor loss: -3.2758283615112305, Critic: 2.9479143619537354\n",
      "Iteration: 25, Score avg: -27.333333333333332, Actor loss: -3.304191827774048, Critic: 3.0580101013183594\n",
      "Iteration: 30, Score avg: -26.666666666666668, Actor loss: -2.903949022293091, Critic: 2.509138584136963\n",
      "Iteration: 35, Score avg: -26.666666666666668, Actor loss: -2.9528462886810303, Critic: 2.70491099357605\n",
      "Iteration: 40, Score avg: -27.666666666666668, Actor loss: -2.740288734436035, Critic: 2.1416704654693604\n",
      "Iteration: 45, Score avg: -27.333333333333332, Actor loss: -2.5955498218536377, Critic: 2.128655195236206\n",
      "Iteration: 50, Score avg: -27.0, Actor loss: -2.6123135089874268, Critic: 2.110088348388672\n",
      "Iteration: 55, Score avg: -26.0, Actor loss: -2.10011887550354, Critic: 1.708368182182312\n",
      "Iteration: 60, Score avg: -27.666666666666668, Actor loss: -2.5616912841796875, Critic: 1.993461012840271\n",
      "Iteration: 65, Score avg: -27.0, Actor loss: -2.4059154987335205, Critic: 1.8511992692947388\n",
      "Iteration: 70, Score avg: -27.0, Actor loss: -2.1820755004882812, Critic: 1.7605119943618774\n",
      "Iteration: 75, Score avg: -27.666666666666668, Actor loss: -2.4246673583984375, Critic: 1.7523192167282104\n",
      "Iteration: 80, Score avg: -26.333333333333332, Actor loss: -1.821115493774414, Critic: 1.5121994018554688\n",
      "Iteration: 85, Score avg: -27.0, Actor loss: -2.056081533432007, Critic: 1.5873494148254395\n",
      "Iteration: 90, Score avg: -26.0, Actor loss: -1.7032808065414429, Critic: 1.4186567068099976\n",
      "Iteration: 95, Score avg: -27.333333333333332, Actor loss: -1.7024677991867065, Critic: 1.147912621498108\n",
      "Iteration: 100, Score avg: -27.333333333333332, Actor loss: -1.6654523611068726, Critic: 1.0783567428588867\n",
      "Iteration: 105, Score avg: -27.0, Actor loss: -1.5465670824050903, Critic: 1.0516586303710938\n",
      "Iteration: 110, Score avg: -27.666666666666668, Actor loss: -2.0325205326080322, Critic: 1.36309814453125\n"
     ]
    }
   ],
   "source": [
    "learn_rate = 0.001\n",
    "n_iters = 1000\n",
    "avg_over = 5\n",
    "\n",
    "init_screen = get_screen(env.reset())\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "actor = Actor(screen_height, screen_width, n_actions).to(device)\n",
    "critic = Critic(screen_height, screen_width, n_actions).to(device)\n",
    "\n",
    "optimizerA = optim.Adam(actor.parameters(), lr=learn_rate)\n",
    "optimizerC = optim.Adam(critic.parameters(), lr=learn_rate)\n",
    "\n",
    "score_avg = []\n",
    "score_avg_all = []\n",
    "a_loss = []\n",
    "a_loss_avg = []\n",
    "c_loss = []\n",
    "c_loss_avg = []\n",
    "\n",
    "for iter in range(n_iters):\n",
    "    state = get_screen(env.reset())\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        #env.render()\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = actor(state), critic(state)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        next_state = get_screen(next_state)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            #print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "            score_avg.append(reward_sum)\n",
    "            break\n",
    "    \n",
    "        \n",
    "    \n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    next_value = critic(next_state)\n",
    "    returns = compute_returns(next_value, rewards, masks)\n",
    "\n",
    "    log_probs = torch.cat(log_probs)\n",
    "    returns = torch.cat(returns).detach()\n",
    "    values = torch.cat(values)\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "    \n",
    "    \n",
    "    a_loss.append(actor_loss)\n",
    "    c_loss.append(critic_loss)\n",
    "    if iter % avg_over == 0 and iter != 0:\n",
    "        score_sum = 0\n",
    "        a_loss_sum = 0\n",
    "        c_loss_sum = 0\n",
    "        for i in range(0,avg_over-1):\n",
    "            score_sum += score_avg[i]\n",
    "            a_loss_sum += a_loss[i]\n",
    "            c_loss_sum += c_loss[i]\n",
    "            n = i\n",
    "        score_sum = score_sum/n\n",
    "        a_loss_sum = a_loss_sum/n\n",
    "        c_loss_sum = c_loss_sum/n\n",
    "        print('Iteration: {}, Score avg: {}, Actor loss: {}, Critic: {}'.format(iter, score_sum, a_loss_sum, c_loss_sum))\n",
    "        score_avg_all.append(score_sum)\n",
    "        a_loss_avg.append(a_loss_sum)\n",
    "        c_loss_avg.append(c_loss_sum)\n",
    "        score_avg.clear()\n",
    "        a_loss.clear()\n",
    "        c_loss.clear()\n",
    "\n",
    "    optimizerA.zero_grad()\n",
    "    optimizerC.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    critic_loss.backward()\n",
    "    optimizerA.step()\n",
    "    optimizerC.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_avg_all)\n",
    "plt.plot(a_loss_avg)\n",
    "plt.plot(c_loss_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 3\n",
    "for iter in range(n_iters):\n",
    "    state = get_screen(env.reset())\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "    env.reset()\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in count():\n",
    "        env.render()\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = actor(state), critic(state)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        next_state = get_screen(next_state)\n",
    "        \n",
    "        reward_sum += reward\n",
    "        log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "        entropy += dist.entropy().mean()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "        masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor, 'results/actor.pkl')\n",
    "torch.save(critic, 'results/critic.pkl')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
